{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 90 input features, 2 hidden layers (5 nodes in each) and two output layers - 0:benign or 1:malignant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('E:\\Online Courses, Internship and more\\edX\\\\breast_cancer_weka_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating the dataset into predictors and response\n",
    "\n",
    "x = df.drop('class',axis = 1)\n",
    "y = df['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dummies for the predictor variables\n",
    "\n",
    "x = pd.get_dummies(x,columns = ['clump_thickness'])\n",
    "x = pd.get_dummies(x,columns = ['uniformity_of_cell_size'])\n",
    "x = pd.get_dummies(x,columns = ['uniformity_of_cell_shape'])\n",
    "x = pd.get_dummies(x,columns = ['marginal_adhesion'])\n",
    "x = pd.get_dummies(x,columns = ['single_epithelial_cell_size'])\n",
    "x = pd.get_dummies(x,columns = ['bare_nuclei'])\n",
    "x = pd.get_dummies(x,columns = ['bland_chromatin'])\n",
    "x = pd.get_dummies(x,columns = ['normal_nucleoli'])\n",
    "x = pd.get_dummies(x,columns = ['mitosis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we  modify the y so that the values 2 and 4 become 0 and 1 respectively\n",
    "\n",
    "for i in range(0,len(y)):\n",
    "    if y.iloc[i] == 2:\n",
    "        y.iloc[i,] = 0\n",
    "    else:\n",
    "        y.iloc[i,] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform our target column into an array of binary numbers for classification\n",
    "\n",
    "y_train = to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(5,activation = 'relu',input_shape=(90,))) # first hidden layer(5 nodes), 90 = number of input features (number of columns in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(5,activation='relu')) # second hidden layer(5 nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output layer - 5 neurons (y is 0 or 1) 0: benign, 1: malignant\n",
    "# we use softmax function so that the sum of the predicted values from all the neurons in the o/p layer sum upto 1\n",
    "model.add(Dense(2,activation='softmax')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "455/455 [==============================] - 0s 309us/step - loss: 0.6896 - accuracy: 0.6637\n",
      "Epoch 2/10\n",
      "455/455 [==============================] - 0s 52us/step - loss: 0.6831 - accuracy: 0.7363\n",
      "Epoch 3/10\n",
      "455/455 [==============================] - 0s 64us/step - loss: 0.6757 - accuracy: 0.7560\n",
      "Epoch 4/10\n",
      "455/455 [==============================] - 0s 44us/step - loss: 0.6660 - accuracy: 0.7846\n",
      "Epoch 5/10\n",
      "455/455 [==============================] - 0s 48us/step - loss: 0.6526 - accuracy: 0.8396\n",
      "Epoch 6/10\n",
      "455/455 [==============================] - 0s 55us/step - loss: 0.6353 - accuracy: 0.8681\n",
      "Epoch 7/10\n",
      "455/455 [==============================] - 0s 42us/step - loss: 0.6116 - accuracy: 0.8967\n",
      "Epoch 8/10\n",
      "455/455 [==============================] - 0s 57us/step - loss: 0.5836 - accuracy: 0.9209\n",
      "Epoch 9/10\n",
      "455/455 [==============================] - 0s 50us/step - loss: 0.5522 - accuracy: 0.9473\n",
      "Epoch 10/10\n",
      "455/455 [==============================] - 0s 44us/step - loss: 0.5208 - accuracy: 0.9495\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x19255c770c8>"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train,epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.56658465, 0.43341535],\n",
       "       [0.56658465, 0.43341535],\n",
       "       [0.20681563, 0.79318434],\n",
       "       [0.56658465, 0.43341535],\n",
       "       [0.56658465, 0.43341535],\n",
       "       [0.56658465, 0.43341535],\n",
       "       [0.32204753, 0.67795247],\n",
       "       [0.56658465, 0.43341535],\n",
       "       [0.19842015, 0.80157983],\n",
       "       [0.56658465, 0.43341535],\n",
       "       [0.56658465, 0.43341535],\n",
       "       [0.56658465, 0.43341535],\n",
       "       [0.56658465, 0.43341535],\n",
       "       [0.38063323, 0.61936677],\n",
       "       [0.5475654 , 0.45243463],\n",
       "       [0.56658465, 0.43341535],\n",
       "       [0.56658465, 0.43341535],\n",
       "       [0.56658465, 0.43341535],\n",
       "       [0.27764693, 0.72235304],\n",
       "       [0.56658465, 0.43341535],\n",
       "       [0.28799653, 0.7120034 ],\n",
       "       [0.17573808, 0.82426196],\n",
       "       [0.24475674, 0.7552433 ],\n",
       "       [0.5269382 , 0.4730618 ],\n",
       "       [0.40968132, 0.5903187 ],\n",
       "       [0.19384967, 0.8061504 ],\n",
       "       [0.2725864 , 0.7274136 ],\n",
       "       [0.24693643, 0.75306356],\n",
       "       [0.12862249, 0.8713776 ],\n",
       "       [0.56658465, 0.43341535],\n",
       "       [0.1933709 , 0.8066291 ],\n",
       "       [0.56658465, 0.43341535],\n",
       "       [0.33050728, 0.6694927 ],\n",
       "       [0.21388495, 0.786115  ],\n",
       "       [0.56658465, 0.43341535],\n",
       "       [0.31861934, 0.6813806 ],\n",
       "       [0.5596174 , 0.44038263],\n",
       "       [0.56081337, 0.43918666],\n",
       "       [0.56658465, 0.43341535],\n",
       "       [0.56658465, 0.43341535],\n",
       "       [0.5603756 , 0.43962446],\n",
       "       [0.56658465, 0.43341535],\n",
       "       [0.56658465, 0.43341535],\n",
       "       [0.56658465, 0.43341535],\n",
       "       [0.17038907, 0.82961094],\n",
       "       [0.22661583, 0.77338415],\n",
       "       [0.56658465, 0.43341535],\n",
       "       [0.56658465, 0.43341535],\n",
       "       [0.56658465, 0.43341535],\n",
       "       [0.56658465, 0.43341535],\n",
       "       [0.56658465, 0.43341535],\n",
       "       [0.56658465, 0.43341535],\n",
       "       [0.36144328, 0.63855666],\n",
       "       [0.56658465, 0.43341535],\n",
       "       [0.204834  , 0.79516596],\n",
       "       [0.25161245, 0.7483876 ],\n",
       "       [0.56658465, 0.43341535],\n",
       "       [0.56658465, 0.43341535],\n",
       "       [0.56658465, 0.43341535],\n",
       "       [0.1716475 , 0.8283525 ],\n",
       "       [0.19889992, 0.8011001 ],\n",
       "       [0.12930854, 0.8706915 ],\n",
       "       [0.56658465, 0.43341535],\n",
       "       [0.56658465, 0.43341535],\n",
       "       [0.56658465, 0.43341535],\n",
       "       [0.24126111, 0.7587389 ],\n",
       "       [0.2665753 , 0.73342466],\n",
       "       [0.2961679 , 0.7038321 ],\n",
       "       [0.17928389, 0.8207161 ],\n",
       "       [0.24629879, 0.75370115],\n",
       "       [0.56658465, 0.43341535],\n",
       "       [0.49771777, 0.5022822 ],\n",
       "       [0.56658465, 0.43341535],\n",
       "       [0.56658465, 0.43341535],\n",
       "       [0.5650862 , 0.4349139 ],\n",
       "       [0.38359964, 0.61640036],\n",
       "       [0.56658465, 0.43341535],\n",
       "       [0.2590216 , 0.74097836],\n",
       "       [0.56658465, 0.43341535],\n",
       "       [0.31231028, 0.68768966],\n",
       "       [0.38020283, 0.61979717],\n",
       "       [0.56658465, 0.43341535],\n",
       "       [0.56658465, 0.43341535],\n",
       "       [0.56658465, 0.43341535],\n",
       "       [0.56658465, 0.43341535],\n",
       "       [0.30632782, 0.69367224],\n",
       "       [0.56658465, 0.43341535],\n",
       "       [0.5638406 , 0.4361594 ],\n",
       "       [0.22827572, 0.7717243 ],\n",
       "       [0.19795316, 0.80204684],\n",
       "       [0.56658465, 0.43341535],\n",
       "       [0.56658465, 0.43341535],\n",
       "       [0.27098984, 0.7290101 ],\n",
       "       [0.3310957 , 0.66890424],\n",
       "       [0.2697358 , 0.73026425],\n",
       "       [0.56658465, 0.43341535],\n",
       "       [0.5646682 , 0.43533185],\n",
       "       [0.56658465, 0.43341535],\n",
       "       [0.3735724 , 0.6264276 ],\n",
       "       [0.19820899, 0.80179095],\n",
       "       [0.56658465, 0.43341535],\n",
       "       [0.24302864, 0.75697136],\n",
       "       [0.29537454, 0.7046254 ],\n",
       "       [0.26276365, 0.7372363 ],\n",
       "       [0.56658465, 0.43341535],\n",
       "       [0.56658465, 0.43341535],\n",
       "       [0.56658465, 0.43341535],\n",
       "       [0.5641321 , 0.43586785],\n",
       "       [0.22908145, 0.7709185 ],\n",
       "       [0.56658465, 0.43341535],\n",
       "       [0.37302113, 0.6269789 ],\n",
       "       [0.56658465, 0.43341535],\n",
       "       [0.24243096, 0.7575691 ],\n",
       "       [0.56658465, 0.43341535]], dtype=float32)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each data-point, the two probabilities should sum to 1 and the higher the probability, the more confident is the\n",
    "# algorithm that a datapoint belongs to the respective class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "175    4\n",
       "369    2\n",
       "153    2\n",
       "270    4\n",
       "94     2\n",
       "      ..\n",
       "173    4\n",
       "530    4\n",
       "258    2\n",
       "111    4\n",
       "484    2\n",
       "Name: class, Length: 114, dtype: int64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
